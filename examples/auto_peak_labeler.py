#!/usr/bin/env python
# -*- coding: utf-8 -*-

'''

This program will read an cxi file generated by psocake's peak finding process,
work on fitting all found peaks with the pseudo voigt 2D model (profile) and
then save the labeled mask to a new 'segmask' dataset in the same cxi file.

'''

import os
import random
import numpy as np
import h5py
import time
import yaml
import argparse

from auto_peak_labeler.app   import PseudoVoigt2DLabeler
from auto_peak_labeler.utils import apply_mask, get_patch_list


# Set up MPI
from mpi4py import MPI
mpi_comm = MPI.COMM_WORLD
mpi_rank = mpi_comm.Get_rank()
mpi_size = mpi_comm.Get_size()
mpi_data_tag = {
##    "num_batch" : 11,
    "input"     : 12,
    "output"    : 13,
    "signal"    : 21,
    "debug"     : 31,
    "sync"      : 41,
}
START_SIGNAL    = 0
TERMINAL_SIGNAL = -1


# Initialize labeler...
labeler = PseudoVoigt2DLabeler()

if mpi_rank == 0:
    # [[[ ARG PARSE ]]]
    parser = argparse.ArgumentParser(description='Process a yaml file.')
    parser.add_argument('yaml', help='The input yaml file.')
    args = parser.parse_args()

    # [[[ CONFIGURE BY YAML ]]]
    fl_yaml = args.yaml
    basename_yaml = fl_yaml[:fl_yaml.rfind('.yaml')]

    # Load the YAML file
    with open(fl_yaml, 'r') as fh:
        config = yaml.safe_load(fh)
    path_cxi_list  = config['cxi']
    win_size       = config['win_size']
    mpi_batch_size = config['mpi_batch_size']

    # ___/ CXI \___
    # Define the keys used below...
    CXI_KEY = { 
        "num_peaks"  : "/entry_1/result_1/nPeaks",
        "data"       : "/entry_1/data_1/data",
        "mask"       : "/entry_1/data_1/mask",
        "peak_y"     : "/entry_1/result_1/peakYPosRaw",
        "peak_x"     : "/entry_1/result_1/peakXPosRaw",
        "segmask"    : "/entry_1/data_1/segmask",
    }

    # Go through each cxi...
    for path_cxi in path_cxi_list:
        with h5py.File(path_cxi, 'r') as fh:
            # Obtain the number of peaks per event...
            k = CXI_KEY['num_peaks']
            num_peaks_by_event = fh.get(k)

            # Go through all hit events...
            for enum_event_idx, num_peaks in enumerate(num_peaks_by_event):
                # Obtain the diffraction image...
                k = CXI_KEY['data']
                img = fh.get(k)[enum_event_idx]

                # Obtain the bad pixel mask...
                k = CXI_KEY['mask']
                mask = fh.get(k)[enum_event_idx]

                # Obtain the Bragg peak positions in this event...
                k = CXI_KEY['peak_y']
                peaks_y = fh.get(k)[enum_event_idx][:num_peaks] # A fixed length array with 0 indicating no peaks, e.g.[2,3,1,..., 0,0,0]

                k = CXI_KEY['peak_x']
                peaks_x = fh.get(k)[enum_event_idx][:num_peaks]

                # Apply mask...
                img = apply_mask(img, 1 - mask, mask_value = 0)

                # Derive image patches...
                patch_list = get_patch_list(peaks_y, peaks_x, img, win_size = win_size)

                # ___/ MPI: MANAGER BROADCAST DATA \___
                # Inform all workers the number of batches to work on...
                batch_patch_list = np.array_split(patch_list, mpi_batch_size)

                # Perform model fitting for each batch...
                res_list = []
                for batch_idx, patch_list_per_batch in enumerate(batch_patch_list):
                    # Split the workfload...
                    patch_list_per_batch_per_chunk = np.array_split(patch_list_per_batch, mpi_size)

                    for i in range(1, mpi_size, 1):
                        # Ask workers to start data process...
                        mpi_comm.send(START_SIGNAL, dest = i, tag = mpi_data_tag["signal"])

                        # Send workers data for processing...
                        data_to_send = patch_list_per_batch_per_chunk[i]
                        mpi_comm.send(data_to_send, dest = i, tag = mpi_data_tag["input"])

                        # Send debug info to workers...
                        batch_size = len(patch_list_per_batch)
                        data_to_send = (enum_event_idx, batch_idx, batch_size)
                        mpi_comm.send(data_to_send, dest = i, tag = mpi_data_tag["debug"])

                    patch_list_current_rank = patch_list_per_batch_per_chunk[0]
                    print(f"E {enum_event_idx:06d}, B {batch_idx:02d}, |C| {len(patch_list_current_rank):04d}({batch_size:04d}), R {mpi_rank:03d}.", flush = True)
                    res_list_current_rank = labeler.fit_all(patch_list_current_rank) 
                    res_list.extend(res_list_current_rank)

                    for i in range(1, mpi_size, 1):
                        res_list_current_rank = mpi_comm.recv(source = i, tag = mpi_data_tag["output"])
                        res_list.extend(res_list_current_rank)

    # Send termination signal...
    for i in range(1, mpi_size, 1):
        mpi_comm.send(TERMINAL_SIGNAL, dest = i, tag = mpi_data_tag["signal"])

else:
    while True:
        received_signal = mpi_comm.recv(source = 0, tag = mpi_data_tag["signal"])

        if received_signal == TERMINAL_SIGNAL: break

        patch_list_current_rank = mpi_comm.recv(source = 0, tag = mpi_data_tag["input"])

        enum_event_idx, batch_idx, batch_size = mpi_comm.recv(source = 0, tag = mpi_data_tag["debug"])

        print(f"E {enum_event_idx:06d}, B {batch_idx:02d}, |C| {len(patch_list_current_rank):04d}({batch_size:04d}), R {mpi_rank:03d}.", flush = True)
        res_list_current_rank = labeler.fit_all(patch_list_current_rank) 

        mpi_comm.send(res_list_current_rank, dest = 0, tag = mpi_data_tag["output"])


mpi_comm.Barrier()

MPI.Finalize()
